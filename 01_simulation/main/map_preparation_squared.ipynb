{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP PREPARATION FOR SPATIALLY-EXPLICIT NEUTRAL MODELS \n",
    "\n",
    "This file requires\n",
    "- scotese shape files for continental boundaries at different times (280, 320 and 340 mya)\n",
    "- info_per_pcoords.csv for location data for each fragment (output of R script)\n",
    "\n",
    "# This file outputs\n",
    "- Raster files for each interval for continental extents\n",
    "- Raster files for sampling regimes across the world\n",
    "- Shapefiles for all sites at each interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from data_preparation import rasterize, extract_from_shapefile, \\\n",
    "\tcreate_shapefile_from_points, find_distance_lat_long, randomly_clear_landscape, add_masks\n",
    "from pycoalescence import Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set up\n",
    "# set this to the path to the data directory. Everything else should work from that\n",
    "# data_directory = \"/Volumes/Seagate 3TB/Paleo/Data/\"\n",
    "data_directory = \"/run/media/sam/Media/Paleo/Data/\"\n",
    "occ_sq_output_dir = os.path.join(data_directory, \"paleo_maps\")\n",
    "# the list of folders containing the shape files to convert to raster\n",
    "csv_directory = \"../../MainSimulationR/input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_csv = pd.read_csv(os.path.join(csv_directory, \"interval_data.csv\"))\n",
    "interval_csv.interval = [x.lower() for x in interval_csv.interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This extracts the continental shapes from the shapefiles into a new file, and then converts it to a\n",
    "# raster.\n",
    "main_dir = os.path.join(data_directory, \"paleo_maps\", \"main\")\n",
    "if not os.path.exists(main_dir):\n",
    "    os.mkdir(main_dir)\n",
    "for folder in set(interval_csv.map_file):\n",
    "\tfile_path = os.path.join(data_directory, \"scotese\", folder, \"wcnt.shp\")\n",
    "\tfile_path_altered = os.path.join(data_directory, \"scotese\", folder, \"wcnt_altered.shp\")\n",
    "\tif not os.path.exists(file_path):\n",
    "\t\traise IOError(\"file {} does not exist\".format(file_path))\n",
    "\t# Drop all features from the shapefile which don't match \"CM\" field\n",
    "\textract_from_shapefile(file_path, file_path_altered, field=\"TYPE\", field_value=\"CM\")\n",
    "\t# Now convert our altered shapefile to a raster\n",
    "\toutput_path = os.path.join(main_dir, \"{}.tif\".format(folder))\n",
    "\trasterize(file_path_altered, output_path, pixel_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the wasteful file\n",
    "for file in os.listdir(main_dir):\n",
    "    os.remove(os.path.join(main_dir, file))\n",
    "os.rmdir(main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the fragment coordinates from csv.\n",
    "info_per_pcoord = pd.read_csv(os.path.join(csv_directory, \"info_per_pcoord_occ.csv\"))\n",
    "info_per_pcoord['lat'] = info_per_pcoord.coord_lat\n",
    "info_per_pcoord['long'] = info_per_pcoord.coord_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the shape file from these points and the spatial sampling masks,\n",
    "#  which are = 1 at our fossil sites, and 0 everywhere else.\n",
    "# These sampling masks will be updated later on to contain the sampling proportion at each site,\n",
    "# not just a binary mask.\n",
    "for interval in info_per_pcoord['interval'].unique():\n",
    "\tfor tet_group in info_per_pcoord['tetrapod_group'].unique():\n",
    "\t\ttmp_df = info_per_pcoord[((info_per_pcoord.interval == interval) &\n",
    "\t\t\t\t\t\t\t\t  (info_per_pcoord.tetrapod_group == tet_group))]\n",
    "\t\t# the mask files for defining our spatial sampling\n",
    "\t\tmask_shp = os.path.join(occ_sq_output_dir,\n",
    "\t\t\t\t\t\t\t\t\"pointsmask_{}_{}.shp\".format(interval.replace(\" \", \"_\").lower(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  tet_group))\n",
    "\t\tmask_raster = os.path.join(occ_sq_output_dir,\n",
    "\t\t\t\t\t\t\t\t\"paleomask_occ_sq_{}_{}.tif\".format(interval.replace(\" \", \"_\").lower(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tet_group))\n",
    "\t\tcreate_shapefile_from_points(tmp_df, mask_shp)\n",
    "\t\t# use our mask to create a binary raster map\n",
    "\t\trasterize(mask_shp, mask_raster, pixel_size=0.01, field=\"prop_ind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_list = set([x.replace(\" \", \"_\").lower() for x in info_per_pcoord[\"interval\"]])\n",
    "tetrapod_g_list = set(info_per_pcoord[\"tetrapod_group\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the origins for each of the time points\n",
    "mask_origins = {}\n",
    "for interval in interval_list:\n",
    "\tfor tet_group in tetrapod_g_list:\n",
    "\t\tmask_raster = os.path.join(occ_sq_output_dir,\n",
    "\t\t\t\t\t\t\t\t\"paleomask_occ_sq_{}_{}.tif\".format(interval, tet_group))\n",
    "\t\tpaleo_mask = Map(mask_raster)\n",
    "\t\t_, _, _,_, x_res, y_res, ulx_mask, uly_mask = paleo_mask.get_dimensions()\n",
    "\t\tmask_origins[(interval, tet_group)] = [uly_mask, ulx_mask, y_res, x_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case the info_per_pcoord\n",
    "info_per_pcoord[\"interval\"] = [x.lower() for x in info_per_pcoord[\"interval\"]]\n",
    "interval_max_ind = info_per_pcoord.groupby([\"interval\", \"tetrapod_group\"],\n",
    "\t\t\t\t\t\t\t\t\t\t   squeeze=True)[\"individuals_total_sq\"].max().reset_index()\n",
    "interval_max_ind.to_csv(os.path.join(csv_directory, \"max_individuals_sq.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_density_dict = {}\n",
    "all_intervals = []\n",
    "all_tetrapod_groups = []\n",
    "for i, row in interval_max_ind.iterrows():\n",
    "\tall_intervals.append(row.interval)\n",
    "\tall_tetrapod_groups.append(row.tetrapod_group)\n",
    "\tmax_density_dict[(row.interval, row.tetrapod_group)] = row.individuals_total_sq\n",
    "all_intervals = set(all_intervals)\n",
    "all_tetrapod_groups = set(all_tetrapod_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(('artinskian', 'amniote'), 75),\n",
       "             (('artinskian', 'amphibian'), 75),\n",
       "             (('asselian', 'amniote'), 30),\n",
       "             (('asselian', 'amphibian'), 30),\n",
       "             (('bashkirian', 'amniote'), 1),\n",
       "             (('bashkirian', 'amphibian'), 115),\n",
       "             (('gzhelian', 'amniote'), 35),\n",
       "             (('gzhelian', 'amphibian'), 35),\n",
       "             (('kasimovian', 'amniote'), 35),\n",
       "             (('kasimovian', 'amphibian'), 16),\n",
       "             (('kungurian', 'amniote'), 75),\n",
       "             (('kungurian', 'amphibian'), 85),\n",
       "             (('moscovian', 'amniote'), 16),\n",
       "             (('moscovian', 'amphibian'), 105),\n",
       "             (('sakmarian', 'amniote'), 30),\n",
       "             (('sakmarian', 'amphibian'), 75)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "OrderedDict(max_density_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the fragment csv for defining fragments within the simulation\n",
    "# This creates one fragment csv for each interval\n",
    "# We now need to create our masks again, altering the values of the shapefile to match our fragment coordinates\n",
    "# This seems a bit stupid, but because fragment coordinates can't be calculated until the mask file\n",
    "# origin is known, this is the best way\n",
    "if not os.path.exists(os.path.join(data_directory, \"configs\")):\n",
    "\tos.mkdir(os.path.join(data_directory, \"configs\"))\n",
    "for interval in interval_list:\n",
    "    for tet_group in tetrapod_g_list:\n",
    "        columns = ['fragment', 'x_min', 'y_min', 'x_max', 'y_max', 'no_individuals']\n",
    "        rows = [x for x in range(0, info_per_pcoord.shape[0], 1)]\n",
    "        fragment_locations = pd.DataFrame(columns=columns, index=rows)\n",
    "        fragment_map = Map(file=os.path.join(occ_sq_output_dir,\n",
    "                                             \"paleomask_occ_sq_{}_{}.tif\".format(interval, tet_group)))\n",
    "        fragment_map.open()\n",
    "        subset_df = info_per_pcoord[(info_per_pcoord[\"interval\"] == interval) &\n",
    "                                     (info_per_pcoord[\"tetrapod_group\"]== tet_group)]\n",
    "        if len(subset_df.index) == 0:\n",
    "            continue\n",
    "        for index, row in subset_df.iterrows():\n",
    "            if row.interval.replace(\" \", \"_\").lower() != interval:\n",
    "                continue\n",
    "            modifier = 0\n",
    "            # # Weirdly one of the fragments is put above instead of below the geo-coord - change it here\n",
    "            # if row.collection_ref == \"174420\":\n",
    "            # \tmodifier = -1\n",
    "            fragment_locations.fragment[index] = row.fragment\n",
    "            offsets = find_distance_lat_long(mask_origins[(interval, tet_group)][0],\n",
    "                                             mask_origins[(interval, tet_group)][1], row.lat, row.long, \n",
    "                                             res=mask_origins[(interval, tet_group)][3])\n",
    "            fragment_locations.x_min[index] = int(math.floor(offsets[1])) \n",
    "            fragment_locations.x_max[index] = int(math.floor(offsets[1] + 1))\n",
    "            fragment_locations.y_min[index] = int(math.floor(offsets[0])) \n",
    "            fragment_locations.y_max[index] = int(math.floor(offsets[0] + 1)) + modifier\n",
    "            fragment_locations.no_individuals[index] = row.individuals_total_sq\n",
    "            this_max_ind = interval_max_ind[(interval_max_ind[\"interval\"] == interval) & \n",
    "                                            (interval_max_ind[\"tetrapod_group\"] == tet_group)].individuals_total_sq\n",
    "            # Alter the fragment mask with updating the sampling values.  ***** add this back in if needed\n",
    "#             try:\n",
    "            # Round up the sampling proportion to the nearest whole number (for the minimum density parameter)\n",
    "            min_density_parameter = 25\n",
    "            expected_value = math.ceil(min_density_parameter*row.individuals_total_sq/this_max_ind)/min_density_parameter\n",
    "            fragment_map.data[fragment_locations.y_min[index], fragment_locations.x_min[index]] = expected_value\n",
    "#             except:\n",
    "#                 continue\n",
    "        fragment_locations = fragment_locations[pd.notnull(fragment_locations['x_min'])]\n",
    "        fragment_locations.to_csv(os.path.join(data_directory, \"configs\", \"fragments_occ_{}_{}.csv\".format(interval,\n",
    "                                                                                                           tet_group)),\n",
    "                                  index=False, header=False)\n",
    "        fragment_map.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the fossil sites that are actually in the sea according to the paleomap.\n",
    "for interval in interval_list:\n",
    "\tinterval_map_path = os.path.join(occ_sq_output_dir, \"{}.tif\".format(interval))\n",
    "\tinterval_map = Map(interval_map_path)\n",
    "\tfor tet_group in tetrapod_g_list:\n",
    "\t\tsample_map_path = os.path.join(occ_sq_output_dir,\n",
    "\t\t\t\t\t\t\t\t  \"paleomask_occ_sq_{}_{}.tif\".format(interval, tet_group))\n",
    "\t\tsample_map = Map(sample_map_path)\n",
    "\t\tsample_map.open()\n",
    "\t\tsubset = np.array(sample_map.data > 0.0).astype(int)\n",
    "\t\tx, y = sample_map.get_x_y()\n",
    "\t\tx_off, y_off = sample_map.calculate_offset(interval_map)\n",
    "\t\tdensity = np.maximum(interval_map.get_subset(x_off, y_off, x, y, no_data_value=0),\n",
    "\t\t\t\t\t\t subset)\n",
    "\t\tinterval_map.write_subset(density, x_off, y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All maps verified\n"
     ]
    }
   ],
   "source": [
    "# Now double check that these all make sense\n",
    "for interval in interval_list:\n",
    "    for tet_group in tetrapod_g_list:\n",
    "        df = pd.read_csv(os.path.join(data_directory, \"configs\", \"fragments_occ_{}_{}.csv\".format(interval, tet_group)),\n",
    "                         header=None)\n",
    "        df.columns = ['fragment', 'x_min', 'y_min', 'x_max', 'y_max', 'no_individuals']\n",
    "        mask_raster = Map(os.path.join(occ_sq_output_dir,\n",
    "                                       \"paleomask_occ_sq_{}_{}.tif\".format(interval, tet_group)))\n",
    "        mask_value = mask_raster.get_cached_subset(0, 0, 1, 1)\n",
    "        this_max_ind = interval_max_ind[(interval_max_ind[\"interval\"] == interval) & \n",
    "                                            (interval_max_ind[\"tetrapod_group\"] == tet_group)].individuals_total_sq\n",
    "        for index, row in df.iterrows():\n",
    "            # print(row.fragment)\n",
    "            mask_value = mask_raster.get_cached_subset(row.x_min, row.y_min, 1, 1)\n",
    "            expected_value = math.ceil(25*row.no_individuals/this_max_ind)/25\n",
    "            if not math.isclose(mask_value[0][0], expected_value, rel_tol=1e-2):\n",
    "                print(\"Mask value not correct: {} != {}\"\n",
    "                      \" for location {}, {} - fragment {}\".format(mask_value[0][0],\n",
    "                                                                  row.no_individuals/this_max_ind,\n",
    "                                                                  row.x_min,\n",
    "                                                                  row.y_min,\n",
    "                                                                  row.fragment))\n",
    "print(\"All maps verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(25*1/100)/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All maps verified\n"
     ]
    }
   ],
   "source": [
    "# Now verify the samplemaps\n",
    "for interval in interval_list:\n",
    "\tinterval_map_path = os.path.join(occ_sq_output_dir, \"{}.tif\".format(interval))\n",
    "\tinterval_map = Map(interval_map_path)\n",
    "\tfor tet_group in tetrapod_g_list:\n",
    "\t\tsample_map_path = os.path.join(occ_sq_output_dir,\n",
    "                                       \"paleomask_occ_sq_{}_{}.tif\".format(interval, tet_group))\n",
    "\t\tsample_map = Map(sample_map_path)\n",
    "\t\tsample_map.open()\n",
    "\t\tsubset = sample_map.data == 0.0\n",
    "\t\tx, y = sample_map.get_x_y()\n",
    "\t\toffset_x, offset_y = sample_map.calculate_offset(interval_map)\n",
    "\t\tdensity = np.ma.array(interval_map.get_subset(offset_x, offset_y, x, y, no_data_value=0),\n",
    "\t\t\t\t\t\t\t  mask=subset)\n",
    "\t\ttot_subset = np.sum(np.invert(subset))\n",
    "\t\ttot_density = np.ma.sum(density)\n",
    "\t\tif tot_subset != tot_density:\n",
    "\t\t\tprint(\"Total from subset: {}\".format(tot_subset))\n",
    "\t\t\tprint(\"Total from density: {}\".format(tot_density))\n",
    "\t\t\traise ValueError(\"Zero density in sampled region found in {}, {}.\".format(interval, tet_group))\n",
    "print(\"All maps verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the fragmented landscapes\n",
    "\n",
    "Generate 20, 40 and 80% fragmented landscapes for times after 305mya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval: kasimovian\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-875541bc904f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfragmented_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \t\t\t\trandomly_clear_landscape(os.path.join(occ_sq_output_dir, \"{}.tif\".format(interval)),\n\u001b[0;32m---> 14\u001b[0;31m \t\t\t\t\t\t\t\t\t\t fragmented_map, proportion_cover)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mfragmented_intervals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfragmented_intervals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PhD/PaleoSampling/Code/PaleoSampling/data_preparation.py\u001b[0m in \u001b[0;36mrandomly_clear_landscape\u001b[0;34m(input_file, output_file, proportion_cover)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mband\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetRasterBand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mband\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReadAsArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# Modify the array based on randomly removing pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/DataPy37/lib/python3.7/site-packages/osgeo/gdal.py\u001b[0m in \u001b[0;36mReadAsArray\u001b[0;34m(self, xoff, yoff, win_xsize, win_ysize, buf_xsize, buf_ysize, buf_type, buf_obj, resample_alg, callback, callback_data)\u001b[0m\n\u001b[1;32m   2764\u001b[0m                                            \u001b[0mresample_alg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample_alg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m                                            \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2766\u001b[0;31m                                            callback_data=callback_data)\n\u001b[0m\u001b[1;32m   2767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2768\u001b[0m     def WriteArray(self, array, xoff=0, yoff=0,\n",
      "\u001b[0;32m~/envs/DataPy37/lib/python3.7/site-packages/osgeo/gdal_array.py\u001b[0m in \u001b[0;36mBandReadAsArray\u001b[0;34m(band, xoff, yoff, win_xsize, win_ysize, buf_xsize, buf_ysize, buf_type, buf_obj, resample_alg, callback, callback_data)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     if BandRasterIONumPy(band, 0, xoff, yoff, win_xsize, win_ysize,\n\u001b[0;32m--> 381\u001b[0;31m                          buf_obj, buf_type, resample_alg, callback, callback_data) != 0:\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/DataPy37/lib/python3.7/site-packages/osgeo/gdal_array.py\u001b[0m in \u001b[0;36mBandRasterIONumPy\u001b[0;34m(band, bWrite, xoff, yoff, xsize, ysize, psArray, buf_type, resample_alg, callback, callback_data)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mBandRasterIONumPy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mband\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbWrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mysize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample_alg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m\"\"\"BandRasterIONumPy(Band band, int bWrite, double xoff, double yoff, double xsize, double ysize, PyArrayObject * psArray, int buf_type, GDALRIOResampleAlg resample_alg, GDALProgressFunc callback=0, void * callback_data=None) -> CPLErr\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_gdal_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBandRasterIONumPy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mband\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbWrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mysize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample_alg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mDatasetIONumPy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbWrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mysize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample_alg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinterleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(data_directory, \"paleo_maps\", \"fragmented\")):\n",
    "\tos.mkdir(os.path.join(data_directory, \"paleo_maps\", \"fragmented\"))\n",
    "fragmented_intervals = []\n",
    "for proportion_cover in [0.2, 0.4, 0.8]:\n",
    "\tfor index, row in interval_csv.iterrows():\n",
    "\t\tif row[\"midpoint\"] < 307:\n",
    "\t\t\tinterval = row[\"interval\"]\n",
    "\t\t\tprint(\"Interval: {}\".format(interval))\n",
    "\t\t\tfragmented_intervals.append(interval)\n",
    "\t\t\tfragmented_map = os.path.join(data_directory, \"paleo_maps\", \"fragmented\",\n",
    "\t\t\t\t\t\t\t\t\t\t  \"{}_{}_fragmented.tif\".format(interval, proportion_cover))\n",
    "\t\t\tif not os.path.exists(fragmented_map):\n",
    "\t\t\t\trandomly_clear_landscape(os.path.join(occ_sq_output_dir, \"{}.tif\".format(interval)),\n",
    "\t\t\t\t\t\t\t\t\t\t fragmented_map, proportion_cover)\n",
    "fragmented_intervals = set(fragmented_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.4\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# Fix the fossil sites that are actually in the sea according to the paleomap,\n",
    "# or have been randomly removed\n",
    "for proportion_cover in [0.2, 0.4, 0.8]:\n",
    "\tprint(proportion_cover)\n",
    "\tfor interval in interval_list:\n",
    "\t\tif interval in fragmented_intervals:\n",
    "\t\t\tinterval_map_path = os.path.join(data_directory, \"paleo_maps\", \"fragmented\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t \"{}_{}_fragmented.tif\".format(interval, proportion_cover))\n",
    "\t\t\tinterval_map = Map(interval_map_path)\n",
    "\t\t\tfor tet_group in tetrapod_g_list:\n",
    "\t\t\t\tsample_map_path = os.path.join(occ_sq_output_dir,\n",
    "                                               \"paleomask_occ_sq_{}_{}.tif\".format(interval, tet_group))\n",
    "\t\t\t\tsample_map = Map(sample_map_path)\n",
    "\t\t\t\tsample_map.open()\n",
    "\t\t\t\tsubset = np.array(sample_map.data > 0.0).astype(int)\n",
    "\t\t\t\tx, y = sample_map.get_x_y()\n",
    "\t\t\t\tx_off, y_off = sample_map.calculate_offset(interval_map)\n",
    "\t\t\t\tdensity = np.maximum(interval_map.get_subset(x_off, y_off, x, y, no_data_value=0),\n",
    "\t\t\t\t\t\t\t\t subset)\n",
    "\t\t\t\tinterval_map.write_subset(density, x_off, y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All maps verified\n"
     ]
    }
   ],
   "source": [
    "# Now verify the samplemaps\n",
    "for proportion_cover in [0.2, 0.4, 0.8]:\n",
    "\tfor interval in interval_list:\n",
    "\t\tif interval in fragmented_intervals:\n",
    "\t\t\tinterval_map_path = os.path.join(data_directory, \"paleo_maps\", \"fragmented\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t \"{}_{}_fragmented.tif\".format(interval,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   proportion_cover))\n",
    "\t\t\tinterval_map = Map(interval_map_path)\n",
    "\t\t\tfor tet_group in tetrapod_g_list:\n",
    "\t\t\t\tsample_map_path = os.path.join(occ_sq_output_dir,\n",
    "                                               \"paleomask_occ_sq_{}_{}.tif\".format(interval, tet_group))\n",
    "\t\t\t\tsample_map = Map(sample_map_path)\n",
    "\t\t\t\tsample_map.open()\n",
    "\t\t\t\tsubset = sample_map.data == 0.0\n",
    "\t\t\t\tx, y = sample_map.get_x_y()\n",
    "\t\t\t\toffset_x, offset_y = sample_map.calculate_offset(interval_map)\n",
    "\t\t\t\tdensity = np.ma.array(interval_map.get_subset(offset_x, offset_y, x, y, no_data_value=0),\n",
    "\t\t\t\t\t\t\t\t\t  mask=subset)\n",
    "\t\t\t\ttot_subset = np.sum(np.invert(subset))\n",
    "\t\t\t\ttot_density = np.ma.sum(density)\n",
    "\t\t\t\tif tot_subset != tot_density:\n",
    "\t\t\t\t\tprint(\"Total from subset: {}\".format(tot_subset))\n",
    "\t\t\t\t\tprint(\"Total from density: {}\".format(tot_density))\n",
    "\t\t\t\t\traise ValueError(\"Zero density in sampled region found in {}, {}\"\n",
    "\t\t\t\t\t\t\t\t\t \" with {} proportion cover.\".format(interval, tet_group, proportion_cover))\n",
    "print(\"All maps verified\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
